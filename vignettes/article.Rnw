%\VignetteIndexEntry{bbl: Boltzmann Bayes Learner}
%\documentclass[article]{jss}
\documentclass[nojss]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{thumbpdf,lmodern,amsmath,bbm}
\DeclareMathOperator*{\argmax}{arg\:max}

%% another package (only for this demo article)
\usepackage{framed}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}
\usepackage[noae]{Sweave}
\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE)
@


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Jun Woo\\University of Minnesota, Minneapolis
   \And Jinhua Wang\\University of Minnesota, Minneapolis}
\Plainauthor{Jun Woo, Jinhua Wang}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
%\title{A Short Demo Article: Regression Models for Count Data in \proglang{R}}
\title{{\bf bbl}: Boltzmann Bayes Learner for High-Dimensional Inference with
Discrete Predictors in \proglang{R}}
\Plaintitle{bbl: Boltzmann Bayes Learner for High-Dimensional Inference with 
Discrete Predictors in R}
\Shorttitle{{\bf bbl}: Boltzmann Bayes Learner in \proglang{R}}

%% - \Abstract{} almost as usual
\Abstract{
  Non-regression-based inferences, such as discriminant analysis, 
  can account for the effect of predictor distributions that may be 
  significant in big data modeling. We describe \pkg{bbl}, an \proglang{R}
  package for Boltzmann Bayes learning, which enables a comprehensive
  supervised learning of the association between a large number of 
  discrete factors and multi-level response variables. Its basic underlying
  statistical model is a collection of (fully visible) Boltzmann machines
  inferred for each distinct response level. The algorithm reduces to the
  naive Bayes learner when interaction is ignored. We illustrate example use
  cases for various scenarios, ranging from modeling of a relatively small
  set of factors with heterogeneous levels to those with hundreds or more
  predictors with uniform levels such as image or genomic data. 
  We show how \pkg{bbl} explicitly quantifies the extra power provided by
  interactions via higher predictive performance of the model. In comparison
  to deep learning-based methods such as restricted Boltzmann machines,
  \pkg{bbl}-trained models can be interpreted directly via their 
  bias and interaction parameters.
}

\Keywords{Supervised learning, Boltzmann machine, naive Bayes, discriminant analysis, \proglang{R}}
\Plainkeywords{Supervised learning, Boltzmann machine, naive Bayes, discriminant analysis, R}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Jun Woo ({\it corresponding author}), Jinhua Wang\\
  Institute for Health Informatics\\
  \emph{and}\\
  Masonic Cancer Center\\
  University of Minnesota\\
  Minneapolis, Minnesota, USA\\
  E-mail: \email{jwoo@umn.edu}
}

\begin{document}
\SweaveOpts{concordance=TRUE}

%\section[Introduction: Count data regression in R]{Introduction: Count data regression in \proglang{R}} \label{sec:intro}
\section{Introduction}\label{sec:intro}

Many supervised learning tasks involve modeling discrete response
variables $y$ using predictors ${\bf x}$ that can occupy discrete factor levels \citep{hastie_etal}.
Ideally, it would be best to model the joint distribution $P({\bf x},y)$
via maximum likelihood, 
\begin{equation}
{\hat \Theta} = \argmax_\Theta \left[\ln P({\bf x},y|\Theta)\right],
\end{equation}
to find parameters $\Theta$. Regression-based methods use 
$P({\bf x},y)=P(y|{\bf x})P({\bf x})\simeq P(y|{\bf x})$. Their advantages include
the wealth of information provided for significance of fit coefficients from rigorous formal results.
An alternative is to use $P({\bf x},y)=P({\bf x}|y)P(y)$ and fit $P({\bf x}|y)$. 
Since $y$ is low-dimensional, this approach could capture extra
information not accessible from regression when there are many covarying predictors.
To make predictions for $y$ using $P({\bf x}|y)$, one uses the Bayes' formula.
Examples include linear and quadratic discriminant analyses 
\citep[pp.~106-119]{hastie_etal} for continuous ${\bf x}$. For discrete ${\bf x}$, naive Bayes 
is the simplest approach, where the covariance among ${\bf x}$ is ignored 
via
\begin{equation}
P({\bf x}|y)\simeq \prod_i P(x_i|y)
\label{eq:nbayes}
\end{equation}
with ${\bf x}=(x_1,\cdots,x_m)$.

In this paper, we focus on supervised learners taking into account the 
high-dimensional nature of $P({\bf x}|y)$ beyond the naive Bayes-level description given by Eq.~(\ref{eq:nbayes}). Namely, 
a suitable parametrization is provided by the Boltzmann machine 
\citep{ackley_etal}, which for the simple binary predictor $x_i=0,1$,
\begin{equation}
P({\bf x}|y)=\frac{1}{Z_y}\exp\left(\sum_i h_i^{(y)}x_i + \sum_{i<j} J_{ij}^{(y)}x_ix_j\right),
\label{eq:boltzmann}
\end{equation}
where $Z_y$ is the normalization constant, or partition function. Equation~(\ref{eq:boltzmann}) 
is the Gibbs distribution for Ising-type models in statistical mechanics \citep{chandler}. 
The two sets of parameters $h_i^{(y)}$ and $J_{ij}^{(y)}$ each represent single variable and 
two-point interaction effects, respectively. 
When the latter vanishes, the model leads to the naive Bayes classifier.
Although exact inference of Eq.~(\ref{eq:boltzmann}) from data is in general not possible, recent
developments led to many accurate and practically usable approximation schemes 
\citep{hyvarinen, morcos_etal,nguyen_etal, nguyen_wood-ieee,nguyen_wood}, 
making its use in supervised learning a viable alternative to regression methods. 
Two approximation methods available for use are pseudo-likelihood inference \citep{besag}
and mean field theory \citep{chandler,nguyen_etal}.

A recently described package \pkg{BoltzMM} can fit the (`fully visible') Boltzmann machine given by 
Eq.~(\ref{eq:boltzmann}) to data using pseudo-likelihood inference \citep{BoltzMM,jones_etal}. 
In contrast, classifiers based on this class of models remain largely unexplored. Supervised learners using 
statistical models of the type (\ref{eq:boltzmann}) usually take the form 
of the {\it restricted} Boltzmann machines instead \citep{hinton}, 
where (visible) predictors are augmented by hidden units and interactions 
are zero except between visible and hidden units. 
The main drawback of such layered Boltzmann machine learners, as is common in all deep learning
algorithms, is the difficulty in interpreting trained models. In contrast, with the fully visible architecture, 
$J_{ij}^{(y)}$ in Eq.~(\ref{eq:boltzmann}), if inferred with 
sufficient power while avoiding overfitting, has direct interpretation of interaction between two variables.

We refer to such learning/prediction algorithms using a generalized version
of Eq.~(\ref{eq:boltzmann}) as Boltzmann Bayes (BB) inference.
An implementation specific to genomic single-nucleotide polymorphism (SNP) data (two response groups, e.g., case and control, and uniform three-level
predictors, i.e., allele counts of $x_i= 0,1,2$) has been reported 
previously \citep{woo_etal-2016}. However, this \proglang{C++} software 
was geared specifically toward genome-wide association studies and is not
suitable for use in more general settings. 
We introduce an R package \pkg{bbl} (Boltzmann Bayes Learner), which 
uses both \proglang{R} and \proglang{C++} for usability and performance,
allowing the user to train and test statistical models in a variety of 
different usage settings.

\section{Model and algorithm} 
For completeness and for reference to software features described in Sec.~\ref{sec:usage}, 
we summarize in this section key relevant formulas \citep{woo_etal-2016} used by \pkg{bbl}, 
generalized such that predictors each can have varying number of factor levels. 

\subsection{Model description}
The discrete response $y_k$ for an instance $k$ takes factor values $y$ among $K\ge 2$ groups; e.g.
$y=\text{\code{case},\:\code{control}}$; $k=1,\cdots, n$ denotes sample index
with the total sample size $n$. We use symbol $y$ for a particular factor value and 
generic response variables interchangeably. The overall likelihood is
\begin{equation}
L=\sum_k \ln P({\bf x}^k,y_k)=\sum_y\sum_{k\in y} \ln P({\bf x}^k,y)\equiv \sum_y L_y,
\end{equation}
where the second summation is over all $k$ for which $y_k=y$.
The inference is first performed for each group $y$ separately, maximizing $L_y$ given by
\begin{equation}
L_y = \sum_{k \in y} \left[\ln P({\bf x}^k|y)+\ln P(y)\right]
=\sum_{k \in y}\ln P({\bf x}^k|y)+n_y p_y,
\label{eq:Ly}
\end{equation}
where $p_y\equiv P(y)$ is the marginal distribution of $y$ and $n_y$ is the size of group $y$.

In parametrizing the first term in Eq.~(\ref{eq:Ly}), we assume that predictor variables take discrete factor levels, each with distinct effect on responses,
e.g., $x_i=\text{\code{a},\:\code{t},\:\code{g},\:\code{c}}$ for DNA sequence data.
The group-specific predictor distribution can then be written as
\begin{equation}
P({\bf x}|y)=\frac{1}{Z_y} \exp\left[ \sum_i h_i^{(y)}(x_i) + \sum_{i<j} J_{ij}^{(y)}(x_i,x_j)\right].
\label{eq:pxy}
\end{equation}
The number of parameters (d.f.) per group $y$ in $\Theta_y=\{h_i^{(y)}(x), J_{ij}^{(y)}(x,x^\prime)\}$ is 
\begin{equation}
{\rm d.f.}=\sum_i (L_i-1) + \sum_{i<j} (L_i-1)(L_j-1),
\end{equation}
where $L_i$ is the total number of levels in factor $x_i$, which contributes one less parameters to d.f. because one of the factors can be taken as reference with the rest measured against it. Internally, 
\pkg{bbl} orders factors, assigns codes $a_i=0,\cdots, L_i-1$, and set $h_i^{(y)}(a_i)=
J_{ij}^{(y)}(a_i,a_j)=0$ whenever $a_i=0$ or $a_j=0$.
We refer to $h_i^{(y)}(x)$ and $J_{ij}^{(y)}(x,x^\prime)$ as bias and interaction 
parameters, respectively.

In the special case where predictor levels are binary ($x_i=0,1$), one may use
the spin variables $s_i=2x_i-1=\pm 1$, as in the package \pkg{BoltzMM} \citep{BoltzMM}.
Its distribution \citep{jones_etal}
\begin{equation}
P({\bf s})\propto \exp\left( \frac{1}{2}{\bf s}^\mathsf{T}\, {\bf M}\, {\bf s} + {\bf b}^\mathsf{T} {\bf s}\right)
\label{eq:boltzmm}
\end{equation}
is then related to Eq.~(\ref{eq:boltzmann}) by
\begin{subequations}
\begin{eqnarray}
b_i &=& \frac{h_i}{2} + \frac{1}{4}\sum_{j\ne i} J_{ij},\\
M_{ij} &=& \frac{1}{4} J_{ij},
\end{eqnarray}
\end{subequations}
where parameter superscripts were omitted because response group is not present.

\subsection{Pseudo-likelihood inference}
One option for fitting Eq.~(\ref{eq:pxy}) to data is pseudo-likelihood maximization \citep{besag}:
\begin{equation}
L_y-n_yp_y=\sum_{k \in y} \ln P({\bf x}^k|y)\simeq \sum_{k\in y} \sum_i \ln P_i(x_i^k|y,x_{j\backslash i}^k)
\equiv \sum_i L_{iy},
\label{eq:Ly2}
\end{equation}
where the effective univariate distribution is conditional to all other predictor values:
\begin{equation}
P_i(x|y,x_{j\backslash i})=\frac{e^{{\bar h}_i^{(y)}(x|x_{j\backslash i})}}{Z_{iy}(x_{j\backslash i})},
\end{equation}
\begin{equation}
Z_{iy}(x_{j\backslash i})
=\sum_x e^{{\bar h}_i^{(y)}(x|x_{j\backslash i})}=1+\sum_{a=1}^{L_i-1} e^{{\bar h}_i^{(y)}(a|x_{j\backslash i})},
\label{eq:ziy}
\end{equation}
and 
\begin{equation}
{\bar h}_i^{(y)}(x|x_{j\backslash i})=h_i^{(y)}(x) + \sum_{j\ne i} J_{ij}^{(y)}(x,x_j).
\label{eq:barh}
\end{equation}

Including $L_2$ penalizers $(\lambda_h,\lambda)$, 
$L_{iy}$ in Eq.~(\ref{eq:Ly2}) becomes
\begin{equation}
L_{iy}=\sum_{k\in y} \left[{\bar h}_i^{(y)}(x_i^k|x_{j\backslash i}^k)-\ln Z_{iy}(x_{j\backslash i}^k)\right]
-\frac{1}{2}\left[\lambda_h \sum_x h_i^{(y)}(x)^2
+\lambda\sum_{j,x,x^\prime} J_{ij}^{(y)}(x,x^\prime)^2\right]
\label{eq:Liy}
\end{equation}
with first derivatives
\begin{subequations}
\begin{eqnarray}
\frac{\partial L_{iy}/n_y}{\partial h_i^{(y)}(x)} &=& {\hat f}_i^{(y)}(x) 
- \frac{1}{n_y}\sum_{k\in y} P_i(x|y, x_{l\backslash i}^k)
-\lambda_h h_i^{(y)}(x),
\\
\frac{\partial L_{iy}/n_y}{\partial J_{ij}^{(y)}(x,x^\prime)} &=& {\hat f}_{ij}^{(y)}(x,x^\prime) 
- \frac{1}{n_y}\sum_{k\in y} \mathbbm{1}(x_j^k=x^\prime)
P_i(x|y, x_{l\backslash i}^k)
-\lambda J_{ij}^{(y)}(x,x^\prime),
\end{eqnarray}
\label{eq:partial}
\end{subequations}
\!\!where ${\hat f}_i^{(y)}(x)$ and ${\hat h}_{ij}^{(y)}(x,x^\prime)$ are the first 
and second moments of predictor values and $\mathbbm{1}(x)$ is the indicator function.
In \pkg{bbl}, Eqs.~(\ref{eq:partial}) are solved in \proglang{C++} functions using the quasi-Newton optimization function 
\code{gsl_multimin_fdfminimizer_vector_bfgs2} in GNU Scientific Library 
(\url{https://www.gnu.org/software/gsl}).
By default, $\lambda_h=0$ and only interaction parameters are penalized.
As can be seen from the third equality of Eq.~(\ref{eq:Ly2}), the pseudo-likelihood inference
decouples into individual predictors, and the inference for each $i$ in \pkg{bbl} is performed
sequentially. The resulting interaction parameters, however, do not satisfy the required
symmetry,
\begin{equation}
J_{ij}(x,x^\prime)=J_{ji}(x^\prime,x).
\end{equation}
After pseudo-likelihood inference, therefore, the interaction parameters are symmetrized as follows:
\begin{equation}
J_{ij}(x,x^\prime)\leftarrow \frac{1}{2}\left[ J_{ij}(x,x^\prime)+ J_{ji}(x^\prime, x)\right].
\end{equation}

In \pkg{bbl}, the input data are filtered such that predictors with only one
factor level (no variation in observed data) are removed. Nevertheless, in cross-validation
of the processed data, sub-divisions into training and validation sets
may lead to instances where factor levels observed for a given predictor within $x_i$ in 
Eq.~(\ref{eq:partial}) are only a subset of those in the whole data.  
It is thus possible that optimization based on Eqs.~(\ref{eq:partial}) is 
ill-defined when any of the predictors are constant. 
In such cases, we augment the training data by an extra instance, in which constant predictors take other factor levels.

\subsection{Mean field inference}
The other option for predictor distribution inference is mean field approximation. In 
data-driven inference, the interaction parameters are approximated as \citep{nguyen_etal}
\begin{equation}
{\hat J}_{ij}^{(y)}(x,x^\prime) = -\left[\mathsf{C}^{(y)}\right]^{-1}_{ij}(x,x^\prime),
\label{eq:mf1}
\end{equation}
i.e., negative inverse of the covariance matrix,
\begin{equation}
\mathsf{C}^{(y)}_{ij}(x,x^\prime) = {\hat f}_{ij}(x,x^\prime) - {\hat f}_i(x){\hat f}_j(x^\prime).
\label{eq:Cij}
\end{equation}
Equation~(\ref{eq:mf1}) can be interpreted as treating discrete ${\bf x}$ as if it were multivariate normal:
Eq.~(\ref{eq:pxy}) would then be the counterpart of the multivariate normal p.d.f. with 
$-J_{ij}^{(y)}(x,x^\prime)$ corresponding to the precision matrix.
In real data where $n \sim {\rm d.f.}$ or less, the matrix inversion is often ill-behaved. It is regularized by 
interpolation of $\mathsf{C}^{(y)}$ 
between non-interacting (naive Bayes)
($\epsilon=0$) and fully interacting limits ($\epsilon=1$):
\begin{equation}
\mathsf{C}^{(y)}  \leftarrow \mathsf{\bar C}^{(y)}=(1-\epsilon)\frac{{\rm Tr}\:\mathsf{C}^{(y)}}
{{\rm Tr}\:\mathsf{I}}{\mathsf I} + \epsilon\mathsf{C}^{(y)},
\end{equation}
where $\mathsf{I}$ is the identity matrix of the same dimension as $\mathsf{C}^{(y)}$.
The parameter $\epsilon$ serves as a good handle for probing the relative importance of interaction 
effects.

The bias parameters are given in mean field by an analog of Eq.~(\ref{eq:barh}),
\begin{equation}
{\hat h}_i^{(y)}(x)=
{\bar h}_i^{(y)}(x)
-\sum_{j\ne i}\sum_{x^\prime}{\hat J}_{ij}^{(y)}(x,x^\prime){\hat f}_j^{(y)}(x^\prime),
\label{eq:mf}
\end{equation}
and 
\begin{equation}
{\bar h}_i^{(y)}(x)=\ln \left[{\hat f}_i^{(y)}(x)/{\hat f}_i^{(y)}(0)\right],
\label{eq:hbari}
\end{equation}
where ${\hat f}_i^{(y)}(0)$ is the frequency of (reference) factor $x_i$ for which the parameters are zero
($a_i=0$).
Equation~(\ref{eq:mf}) relates the effective bias for predictor $x_i$ (the first term on the right)
as the sum of univariate bias (left-hand side) and combined mean effects of interactions with other
variables (the second term on the right) \citep{chandler}.
The effective bias is related to frequency via Eq.~(\ref{eq:hbari}) because
\begin{equation}
{\hat f}_i^{(y)}(x)
=\frac{e^{{\bar h}_i^{(y)}(x)}}{Z_{iy}}={\hat f}_i^{(y)}(0)\,e^{{\bar h}_i^{(y)}(x)}
\label{eq:hat}
\end{equation}
where the fact that ${\bar h}_i^{(y)}(0)=0$  was used in the second equality.

As in pseudo-likelihood maximization, mean field inference also may encounter non-varying predictors
during cross-validation. To apply the same inference scheme using
Eqs.~(\ref{eq:Cij}), (\ref{eq:mf}) and (\ref{eq:hbari}) to such cases, 
the single-variable
frequency ${\hat f}_i^{(y)}(x)$ and covariance ${\hat f}_{ij}^{(y)}(x,x^\prime)$ are computed 
using data augmented by a prior count of 1 uniformly distributed among all $L_i$ factor levels for 
each predictor.

\subsection{Classification}
For prediction, we combine predictor distributions for all response groups via Bayes formula:
\begin{equation}
P(y|{\bf x})=\frac{P({\bf x}|y)p_y}{\sum_{y^\prime}P({\bf x}|y^\prime) p_{y^\prime}}
=\frac{1}{1+\sum_{y^\prime\ne y} P({\bf x}|y^\prime) p_{y^\prime}/
P({\bf x}|y) p_y}
=\frac{1}{1+e^{-F_y({\bf x})}},
\label{eq:py}
\end{equation}
where 
\begin{equation}
F_y({\bf x})=\ln\left[\frac{P({\bf x}|y)p_y}
{\sum_{y^\prime \ne y}P({\bf x}|y^\prime)p_{y^\prime}}\right].
\label{eq:Fy}
\end{equation}
For binary response coded as $y=0,1$, Eq.~(\ref{eq:Fy}) reduces to
\begin{eqnarray}
F_1({\bf x})&=&\ln P({\bf x}|y=1)-\ln P({\bf x}|y=0) + \ln (p_1/p_0)\nonumber\\
&=& \sum_i \left[h_i^{(1)}(x_i)-h_i^{(0)}(x_i)\right]
+\sum_{i<j} \left[ J_{ij}^{(1)}(x_i,x_j) -J_{ij}^{(0)}(x_i,x_j)\right]+
\ln \frac{Z_0 p_1}{Z_1p_0}.
\label{eq:F}
\end{eqnarray}
Therefore, if $J_{ij}^{(y)}(x,x^\prime)=0$ (naive Bayes), Eq.~(\ref{eq:py}) takes the form of
the logistic regression formula. However, the actual naive Bayes parameter values differ from logistic 
regression fit. No expression for $P(y|{\bf x})$ simpler than Eq.~(\ref{eq:py}) exists for data with more 
than two groups.

In pseudo-likelihood maximization inference, $Z_y$ can be approximated by
\begin{equation}
\ln Z_y = \frac{1}{n_y}\sum_{k\in y}\sum_i 
\ln \left\{\sum_x \left[e^{h_i^{(y)}(x)+\sum_{j\ne i}J_{ij}(x,x_j^k)/2}\right]\right\},
\label{eq:lnz1}
\end{equation}
or with the same expression without the factor of $1/2$ in the interaction 
term in the exponent (default). This quantity can be conveniently computed 
during the optimization process. With the mean field option, the following 
expression is used:
\begin{equation}
\ln Z_y = -\ln {\hat f}^{(y)}(0) - \frac{1}{2}\sum_{i\ne j} \sum_{x,x^\prime}J_{ij}(x,x^\prime) {\hat f}_i(x)
{\hat f}_j(x^\prime).
\label{eq:lnzy}
\end{equation}

For a test data set for which the actual group identity $y_k$ of data 
instances are known, the prediction score (accuracy) may be defined as
\begin{equation}
s=\frac{1}{n}\sum_k \mathbbm{1}\left[{\hat y}({\bf x}^k)=y_k\right],
\label{eq:s}
\end{equation}
where 
\begin{equation}
{\hat y}({\bf x}) = \argmax_y P(y|{\bf x}).
\label{eq:yhat}
\end{equation}
If response is binary, the score defined by Eq.~(\ref{eq:s}) is sensitive to marginal distributions of 
the two groups via Eq.~(\ref{eq:F}). 
The area under curve (AUC) of receiver operating characteristic is a more robust performance measure
independent of probability cutoff. In \pkg{bbl}, the prediction score
given by Eqs.~(\ref{eq:s}) and (\ref{eq:yhat}) is used in general with the option to use AUC for binary
response using R package \pkg{pROC} \citep{proc}.

\section{Software Usage and Tests}
\label{sec:usage}
\subsection[Titanic data]{\code{Titanic} data}
We first illustrate BB inference for full multilevel response data sets by \pkg{bbl} using the base 
\proglang{R} data \code{Titanic}:
%
<<data>>=
titanic <- as.data.frame(Titanic)
head(titanic)
@
%
The frequency data can be converted into raw data (one observation per row) using the utility function
\code{freq2raw} in \pkg{bbl}:
%
<<raw>>=
library(bbl)
titanic <- freq2raw(titanic, Freq='Freq')
head(titanic)
summary(titanic)
@
%
We first divide the sample into train and test sets,
%
<<div>>=
set.seed(158)
nsample <- nrow(titanic)
flag <- rep(TRUE, nsample)
flag[sample(nsample, nsample/2)] <- FALSE
dtrain <- titanic[flag,]
dtest <- titanic[!flag,]
@
%
and apply linear regression using \code{glm}:
%
<<lr>>=
fit <- glm(Survived ~ ., family=binomial(), data=dtrain)
prl <- predict(fit, newdata=dtest)
pROC::roc(response=dtest$Survived, predictor=prl, direction='<')$auc
@
%

The BB inference in \pkg{bbl} uses objects of \code{S4} class \code{bbl} 
instantiated by input training data:
%
<<class>>=
model <- bbl(data=dtrain, y='Survived')
model
@
%
The argument \code{y} specifies the column name of the response variable.

We first try a single pseudo-likelihood inference by
%
<<ps>>=
model <- train(model, method='pseudo', lambda=0)
model@h
head(model@J,n=1)
@
%
The function \code{train} here solves the maximum pseudo-likelihood equations (\ref{eq:partial}) 
using training data and stores the inferred parameters
$h_i^{(y)}$ and $J_{ij}^{(y)}$ as lists with argument order $(y,i)$ and 
$(y,i,j)$, respectively. 
The inner-most elements of the lists
are vectors and matrices of dimension $L_i-1=\text{\code{c(3,1,1)}}$ and $(L_i-1,L_j-1)$, respectively.

We predict the survival probability of test individuals using the trained model:
%
<<survival>>=
pr <- predict(model, newdata=dtest, logit=FALSE)
head(pr)
pROC::roc(response=dtest$Survived, predictor=pr[,2], direction='<')$auc
@
%
Here, Eq.~(\ref{eq:py}) was used with ${\bf x}$ from the supplied \code{newdata}.
The \code{newdata} can either contain the response column as in the above
example (the program will disregard it), or only the predictor columns. 
The predictor columns must either have the same order as in the training 
data or be labeled with column names.

One can do cross-validation applied to \code{dtrain} data, dividing it into 
\code{nfold = 5} train/validation subsets of 4:1 proportion, and aggregating
predictions for validation sets using the trained model:
%
<<cv>>=
cv <- crossval(model, method='pseudo', lambda=10^seq(-6,-2,0.5),
               verbose=0)
cv
@
%
It returns a \code{data.frame} of AUCs for multiple \code{lambda} values.
There is a maximum AUC at $\lambda=1\times 10^{-3}$. We use this information
to make prediction:
%
<<pr2>>=
lstar <- cv[cv$auc==max(cv$auc),]$lambda
model <- train(model, method='pseudo', lambda=lstar)
pr2 <- predict(model, newdata=dtest, progress.bar=FALSE)
yhat2 <- model@groups[apply(pr2,1,which.max)]
mean(dtest$Survived==yhat2)
pROC::roc(response=dtest$Survived, predictor=pr2[,2], direction='<')$auc
@
%

\subsection{Simulated data}
We next demonstrate the reliability of \pkg{bbl} inference using simulated data.
%
<<sim1>>=
predictors <- list()
m <- 5
L <- 3
for(i in 1:m) predictors[[i]] <- seq(0, L-1)
par <- randompar(predictors, dh=1, dJ=1, distr='unif')
names(par)
@
%
The utility function \code{randompar} generates random parameters for predictors. We 
have set the total number of predictors as $m=5$, each taking values $0,1,2$ ($L_i=L=3$). 
%
<<sample>>=
xi <- sample_xi(nsample=10000, predictors=predictors, h=par$h, J=par$J, 
                code_out=TRUE)
head(xi)
@
%
The function \code{sample_xi} will list all possible predictor states and sample configurations 
based on the distribution (\ref{eq:pxy}). The total number of states here is $L^m=3^5$, which is amenable for 
exhaustive enumeration. However, this is possible only for small $m$ and $L$.
If either are even moderately larger, \code{sample_xi} will hang.

Because there is only one response group, we call the main engine 
\code{mlestimate} of \pkg{bbl} inference directly instead of \code{train}:
%
<<mle>>=
fit <- mlestimate(xi=xi, method='pseudo',lambda=0)
names(fit)
@
%
In contrast to \code{train}, which is designed for use with \code{bbl}
object with multiple response groups and predictors in factors,
\code{mlestimate} is for a single group and requires input matrix 
\code{xi} whose elements are integral codes of factors: $a_i=0,\cdots,L_i-1$.

Figure~\ref{fig:par} compares the true and inferred parameters. Here, the
sample size was large enough that no regularization was necessary.

\begin{figure}[t!]
\centering
<<par, echo=FALSE, fig=TRUE, height=4.0, width=4.5>>=
oldpar <- par(mar = c(4,4,1,2),lwd=0.5,cex.axis=0.8,cex.lab=1.0,
              mgp=c(2.2,0.9,0),tck=-0.03)
range <- range(par$h, par$J, fit$h, fit$J)
plot(x=unlist(par$h), y=unlist(fit$h), bg='cornflowerblue', xlim=range, yli=range, pch=21,
     cex=0.8, xlab='True', ylab='Inferred', lwd=0.7, xaxt='n',yaxt='n',bty='n')
axis(side=1, at=seq(-1.5,1.5,0.5), lwd=0.5, las=1)
axis(side=2, at=seq(-1.5,1.5,0.5), lwd=0.5, las=1)
segments(x0=-1,x1=1,y0=-1,y1=1, lty=2, lwd=0.7)
points(x=unlist(par$J), y=unlist(fit$J), pch=24, bg='orange', cex=0.8, lwd=0.7)
legend(x=0.5,y=-0.5, legend=expression(italic(h), italic(J)), cex=0.8, pch=c(21,24), pt.bg=c('cornflowerblue',
       'orange'))
par(oldpar)
@
\caption{\small \label{fig:par}
Comparison of true parameters and those inferred from pseudo-likelihood BB inference.
See the text for conditions.}
\end{figure}

We next simulate a full binary response data set with four-level predictors:
%
<<atgc>>=
set.seed(135)
n <- 1000
for(i in 1:m) predictors[[i]] <- c('a','c','g','t')
par <- xi <- list()
for(iy in 1:2){
  par[[iy]] <- randompar(predictors, h0=0.1*(iy-1), J0=0.1*(iy-1),
                         distr='unif')
  xi[[iy]] <- sample_xi(nsample=n, predictors=predictors, h=par[[iy]]$h, 
                        J=par[[iy]]$J)
}
dat <- cbind(rbind(xi[[1]],xi[[2]]), data.frame(y=c(rep('control',n),
                                                    rep('case',n))))
model <- bbl(data=dat, groups=c('control','case'))
model
@
%
The explicit \code{groups} argument to \code{bbl} overrides the default detection and ordering of response groups 
from data. We now cross-validate using mean field inference,
%
<<cr-mf>>=
cv <- crossval(model, method='mf', eps=seq(0,1,0.1),verbose=0)
head(cv)
@
%
Here, \code{train} is called inside \code{crossval} as before but with \code{method = 'mf'}, 
which triggers mean field inference with Eqs.~(\ref{eq:mf1}) and (\ref{eq:mf}).

As shown in Fig.~\ref{fig:mfcv}{\bf a}, prediction AUC is optimized near $\epsilon=0.8$.
The difference between AUC at $\epsilon=0$ (naive Bayes limit) and the maximum is a measure of 
the overall effect of interaction.  We select three values of $\epsilon$ and examine the fit:
%
<<mf-par>>=
fit <- list()
eps <- c(0.2, 0.8, 1.0)
for(i in seq_along(eps))
  fit[[i]] <- train(model, method='mf', eps=eps[i], verbose=0)
@
%

\begin{figure}[t!]
\centering
<<cv, echo=FALSE, fig=TRUE, height=5.5, width=6>>=
oldpar <- par(mfrow=c(2,2),mar = c(4,4,2,2),lwd=0.5,cex.axis=0.8,
              cex.lab=0.9,mgp=c(2.2,0.8,0),tck=-0.03,las=1)
estar <- cv[cv[,2]==max(cv[,2]),1]
plot(x=cv$epsilon, y=cv$auc, type='b',xlab=expression(epsilon),ylab='AUC',lwd=0.7,cex=0.7,bty='n')
segments(x0=estar,x1=estar, y0=0, y1=cv[cv[,1]==estar,2], lty=2, lwd=0.5, col='red')
title(adj=0,cex.main=1.2,font=2,main='a')

range <- c(-1.5, 1.5)
for(i in 1:3){
  plot(x=c(unlist(par[[1]]$h), unlist(par[[2]]$h)),  y=unlist(fit[[i]]@h), 
       bg='cornflowerblue', xlim=range, ylim=range, pch=21,
       cex=0.7, xlab='True', ylab='Inferred', lwd=0.7, xaxt='n',yaxt='n',bty='n')
  axis(side=1, at=seq(-1.5,1.5,0.5), lwd=0.5, las=1)
  axis(side=2, at=seq(-1.5,1.5,0.5), lwd=0.5, las=1)
  segments(x0=-2,x1=2,y0=-2,y1=2, lty=2, lwd=0.7)
  points(x=c(unlist(par[[1]]$J),unlist(par[[2]]$J)), y=unlist(fit[[i]]@J), pch=24, bg='orange', 
         cex=0.7, lwd=0.7)
  if(i==1) legend(x=0.5,y=-0.5, legend=expression(italic(h), italic(J)), cex=0.8, pch=c(21,24), 
           pt.bg=c('cornflowerblue','orange'))
  title(adj=0,main=letters[i+1],cex.main=1.1,font=2)
  mtext(side=3,line=1.0,cex=0.8,bquote(epsilon==.(eps[i])),adj=0.5)
}
par(oldpar)
@
\caption
{\small 
Regularized mean field inference using simulated data.
({\bf a}) Cross-validation AUC with respect to regularization parameter $\epsilon$.
({\bf b}-{\bf d}) Comparison of true and inferred parameters under three $\epsilon$ values. 
Best fit is achieved when AUC is maximum.}
\label{fig:mfcv}
\end{figure}

Figure~\ref{fig:mfcv}{\bf b}-{\bf d} compares the three inferred parameter sets \code{(fit[[i]]@h, fit[[i]]@J)} 
with the true values \code{(par[[iy]]\$h, par[[iy]]\$J)}. As $\epsilon$ increases from 0 to 1, 
interaction parameter $J$ grows from zero to large, usually overfit levels. We verify that
the bias and variance strike the best balance under $\epsilon=0.8$ (Fig.~\ref{fig:mfcv}{\bf c}),
as suggested by cross-validation AUC in Fig.~\ref{fig:mfcv}{\bf a}.

\subsection{Image data}

\begin{figure}[t!]
\centering
\includegraphics[scale=0.8]{mnist_mf}
\caption{\small Cross-validation of BB inference on MNIST data using mean field option.
Sample sizes are for down-sampled example and full data sets, respectively.}
\label{fig:mnist}
\end{figure}

Advantages of \pkg{bbl} over regression become more apparent when dealing with large data sets
and predictors numbering $\sim 100$ or more.
Here, we consider the MNIST data set (\url{yann.lecun.com/exdb/mnist/})
widely used for benchmarking classification algorithms \citep{lecun_etal}.
Each sample in this data set contains grayscale levels ($x_i=[0,255]$) derived from an image of
hand-written digits ($y_k=0,\cdots,9$) for $m=28\times 28=784$ pixels.
We use down-sampled training ($n=1,\!000$) and test($n=500$) data sets, where grayscale has been 
transformed into binary predictors ($x_i=0,1$):
%
<<mnist>>=
dat <- read.csv(system.file('extdata/mnist_train.csv',package='bbl'))
dat[1:5,1:10]
mnist <- bbl(data=dat)
mnist
@
%
Note that when the object is created, the predictors without factor variations
(pixels that are always empty) are dropped from data.
%
<<mnist2, eval=FALSE>>=
cv <- crossval(mnist, method='mf', eps=0.1)
@
%
The above run will take a few minutes. By feeding a vector of $\epsilon$ values, one can obtain
the profile shown in Fig.~\ref{fig:mnist} (white symbols). The substantial jump in 
performance under $\epsilon^*\sim 0.1$ over $\epsilon \rightarrow 0$ (naive Bayes) limit 
gives a measure of interaction effects.
The relatively small value of $\epsilon^*$ at the optimal condition,
compared to e.g., Fig.~\ref{fig:mfcv}{\bf a}, reflects the sparseness of image 
data. 

We now retrain the model without cross-validation under $\epsilon^*$ and classify test set 
(also down-sampled to $n=500$) images:
%
<<mnist3, eval=FALSE>>=
mnist <- train(mnist, method='mf', eps=0.1)
dtest <- read.csv(system.file('extdata/mnist_test.csv',package='bbl'))
dtest <- dtest[,colnames(dtest) %in% colnames(mnist@data)]
pr <- predict(mnist, newdata=dtest[,-1], progress.bar=FALSE)
yhat <- colnames(pr)[apply(pr, 1, which.max)]
mean(yhat==dtest$y)
@
%
Since \code{mnist} dropped a subset of original predictors, the test data must be filtered accordingly. 
Note the increase in test score compared to cross-validation score because of the use of full 
training data. Set \code{progress.bar = TRUE} to monitor the progress in a slow \code{predict} run.

We performed similar cross-validation and test analyses of the full MNIST data 
(training $n=60,\!000$ and test $n=10,\!000$; Fig.~\ref{fig:mnist}, red symbols) 
and obtained the test 
score of $0.932$ (classification error rate $6.8\%$), which compares favorably 
with some of the best-performing large-scale neural network algorithms 
\citep{lecun_etal,salakhutdinov_hinton} (Table.~\ref{tb:mnist}).

\begin{table}[t!]
\centering
\begin{tabular}{llll}
\hline
Algorithm           & Method           & Error rate (\%) & Reference/package   \\ \hline
Linear classifier   & 1-layer NN       & $12.0$        & \cite{lecun_etal}\\
K-nearest neighbors & Euclidean (L2)   & $5.0$         & \cite{lecun_etal}\\
2-layer NN          & 300 hidden units & $4.7$         & \cite{lecun_etal}\\
RBM                 & 2-layer          & $0.95$        & \cite{salakhutdinov_hinton}\\
Naive Bayes         & Mean field ($\epsilon=0$) & $16.2$ &  \pkg{bbl}                 \\
BB                  & Mean field ($\epsilon=0.05$)      & $6.8$         &    \pkg{bbl}               \\
\hline
\end{tabular}
\caption{\small \label{tb:mnist} Performance comparison of BB inference and other models on MNIST data set.
BB, Boltzmann Bayes; NN, neural network; RBM, restricted Boltzmann machine.}
\end{table}

\subsection{Transcription factor binding site data}
\label{sec:tfbs}
One of machine learning tasks of considerable interest in biomedical applications is the 
detection of transcription factor binding sites within genomic sequences \citep{wasserman_sandelin}.
Transcription factors are proteins that bind to specific DNA sequence segments and regulate
gene expression programs.
Public databases, such as JASPAR \citep{jaspar2018}, host known transcription factors and their 
binding sequence motifs. Supervised learners allow users to leverage these data sets and search for
binding motifs from candidate sequences.

Here, we illustrate such an inference using an example set 
(MA0014.3) of binding motif sequences from JASPAR
(\url{http://jaspar.genereg.net}):
%
<<jaspar>>=
seq <- read.fasta(system.file('extdata/MA0014.3.fasta',package='bbl'))
head(seq)
dim(seq)
@
%
The data set consists of common nucleotide segments from $n=948$ raw sequences used for motif discovery.
The function \code{read.fasta} will read a FASTA format file and turn it into a data frame.
We simulate a training set by generating non-binding sequences with random mutation of 3 nucleotides:
%
<<jaspar2>>=
set.seed(561)
nsample <- NROW(seq)
m <- NCOL(seq)
nt <- c('A','C','G','T')
ctrl <- as.matrix(seq)
for(k in seq(nsample))
  ctrl[k, sample(m,3)] <- sample(nt, 3, replace=TRUE)
colnames(ctrl) <- 1:m
data <- rbind(data.frame(y=rep('Binding', nsample), seq), 
              data.frame(y=rep('Non-binding', nsample), ctrl))
data <- data[sample(NROW(data)), ]
@
%
We assess the performance of pseudo-likelihood and mean field inferences below using cross-validation:
%
<<jaspar3>>=
model <- bbl(data=data)
model
ps <- crossval(model, method='pseudo', lambda=10^seq(-1,-2,-0.2), verbose=0)
ps
mf <- crossval(model, method='mf', eps=seq(0.1,0.4,0.1),verbose=0)
mf
@
%
In both cases, there is an optimal, intermediate range of regularization with
maximum AUC (Fig.~\ref{fig:jaspar}). The level of performance attainable with non-interacting models, such as position frequency matrix \citep{wasserman_sandelin}, 
corresponds to the $\epsilon=0$ limit in Fig.~\ref{fig:jaspar}{\bf b}.
The AUC range obtained above is representative of the sensitivity and specificity levels one would get
when scanning a genomic segment using a trained model for detection of a binding site to within 
the resolution of $\sim 3$~base pairs.

\begin{figure}[t!]
\centering
\includegraphics[scale=0.8]{tfbf}
\caption{\small Cross-validation of discrete BB model on transcription factor binding motif data with control sequences generated by 3 nucleotide mutations.
Data set is from \cite{jaspar2018} (sample ID MA0014.3; see text).
({\bf a}) Pseudo-likelihood and ({\bf b}) mean field inferences.}
\label{fig:jaspar}
\end{figure}

We analyzed 856 data sets from JASPAR database of varying sample
sizes and segment lengths with 
the same protocol. Differences between fully optimized AUC scores and those from non-interacting models (naive Bayes) were most pronounced above the
intermediate range of AUC, and were 
independent of segment lengths (Fig.~\ref{fig:tfbs}{\bf a}).
Pseudo-likelihood results had better scores compared to mean field on avarge (Fig.~\ref{fig:tfbs}{\bf b}).

\begin{figure}[t!]
\centering
\includegraphics[scale=1]{tfbs}
\caption{\small 
AUC scores of \pkg{bbl} model trained on 856 transcription factor binding site
sequence data sets from JASPAR \citep{jaspar2018} under the same protocol as in Fig.~\ref{fig:jaspar}.
({\bf a}) Comparison of naive Bayes (NB; mean field with $\epsilon=0$) and full mean field (MF) results. 
({\bf b}) Comparison of mean field (MF) and pseudo-likelihood 
maximization (pseudo-L) scores. The symbol colors show the segment length of each
binding site data (color-map in {\bf a}).
}
\label{fig:tfbs}
\end{figure}


\section{Summary} \label{sec:summary}
We introduced a user-friendly R package \pkg{bbl}, implementing general
BB classifiers applicable to heterogeneous, multifactorial predictor data 
associated with a discrete multi-class response variable.
The currently available \proglang{R} package \pkg{BoltzMM} is limited
to fitting data into a single fully visible Boltzmann distribution
without reference to response variables, and assumes binary 
predictors. The package \pkg{bbl} extends the basic statistical distribution to accommodate 
heterogeneous, factor-valued predictors via 
Eq.~(\ref{eq:pxy}), embedding it in a Bayesian classifier for supervised learning and prediction.

Compared to more widely applied restricted Boltzmann machine algorithms
\citep{hinton}, the BB model explicitly infers interaction parameters 
for all pairs of predictors, making it possible to interpret trained 
models directly. Tests on MNIST suggest performances comparable to other 
deep layer neural network models in 
classification tests. It is especially suited to data types where a 
moderate number of unordered features (such as nucleotide sequences) combine to determine class identity, as in transcription factor binding motifs 
(Sec.~\ref{sec:tfbs}). Among the two options for inference methods, mean field
(\code{method = `mf'}) is faster but can become memory intensive for models 
with a large number of predictors. Pseudo-likelihood maximization 
(\code{method = 'pseudo'}) is slower but generally performs better.

%% -- Optional special unnumbered sections -------------------------------------

\section*{Computational details}

Installation of \pkg{bbl} requires the GNU Scientific library 
\url{https://www.gnu.org/software/gsl} installed. The results in this paper 
were obtained using \proglang{R}~\Sexpr{paste(R.Version()[6:7], collapse = ".")}. \proglang{R} itself 
and all packages used are available from the Comprehensive
\proglang{R} Archive Network (CRAN) at \url{https://CRAN.R-project.org/}.

\bibliography{refs}

%% -- Appendix (if any) --------------------------------------------------------
%% - After the bibliography with page break.
%% - With proper section titles and _not_ just "Appendix".

\newpage

%\begin{appendix}
%
%\section{More technical details} \label{app:technical}
%
%\end{appendix}

%% -----------------------------------------------------------------------------

\end{document}
